{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":91198,"databundleVersionId":10884264,"sourceType":"competition"},{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install torchinfo\n#!pip install scikit-image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.198410Z","iopub.execute_input":"2025-02-18T06:21:07.198795Z","iopub.status.idle":"2025-02-18T06:21:07.202979Z","shell.execute_reply.started":"2025-02-18T06:21:07.198766Z","shell.execute_reply":"2025-02-18T06:21:07.201823Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\n#from torch.nn.Functional\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport cv2 as cv\nfrom torchinfo import summary\nimport einops\nfrom skimage import feature\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.205011Z","iopub.execute_input":"2025-02-18T06:21:07.205258Z","iopub.status.idle":"2025-02-18T06:21:07.216425Z","shell.execute_reply.started":"2025-02-18T06:21:07.205234Z","shell.execute_reply":"2025-02-18T06:21:07.215400Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths to dataset files\npath = '/kaggle/input/ai-vs-human-generated-dataset'\ntrain_csv = '/kaggle/input/detect-ai-vs-human-generated-images/train.csv'\ntest_csv = '/kaggle/input/detect-ai-vs-human-generated-images/test.csv'\n\n# Load the training and test datasets\ntrain = pd.read_csv(train_csv)\ntest = pd.read_csv(test_csv)\n\n# Print dataset shapes\nprint(f'Training dataset shape: {train.shape}')\nprint(f'Test dataset shape: {test.shape}')\n\n# Preprocess column names for consistency\ntrain = train[['file_name', 'label']]\ntrain.columns = ['id', 'label']\n\n# Display columns for reference\nprint(\"Train columns:\", train.columns)\nprint(\"Test columns:\", test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.217788Z","iopub.execute_input":"2025-02-18T06:21:07.218013Z","iopub.status.idle":"2025-02-18T06:21:07.321531Z","shell.execute_reply.started":"2025-02-18T06:21:07.217990Z","shell.execute_reply":"2025-02-18T06:21:07.320182Z"}},"outputs":[{"name":"stdout","text":"Training dataset shape: (79950, 3)\nTest dataset shape: (5540, 1)\nTrain columns: Index(['id', 'label'], dtype='object')\nTest columns: Index(['id'], dtype='object')\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"print(\"To check the data distribution for training\")\ntrain['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.322743Z","iopub.execute_input":"2025-02-18T06:21:07.322975Z","iopub.status.idle":"2025-02-18T06:21:07.330431Z","shell.execute_reply.started":"2025-02-18T06:21:07.322950Z","shell.execute_reply":"2025-02-18T06:21:07.329388Z"}},"outputs":[{"name":"stdout","text":"To check the data distribution for training\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"label\n1    39975\n0    39975\nName: count, dtype: int64"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# Split the training data into training and validation sets (95% train, 5% validation)\ntrain_df, val_df = train_test_split(\n    train, \n    test_size=0.05, \n    random_state=42,  \n    stratify=train['label'] \n)\n\n# Print shapes of the splits\nprint(f'Train shape: {train_df.shape}')\nprint(f'Validation shape: {val_df.shape}')\n\n# Check class distribution in both sets\nprint(\"\\nTrain class distribution:\")\nprint(train_df['label'].value_counts(normalize=True))\n\nprint(\"\\nValidation class distribution:\")\nprint(val_df['label'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.331555Z","iopub.execute_input":"2025-02-18T06:21:07.331801Z","iopub.status.idle":"2025-02-18T06:21:07.369765Z","shell.execute_reply.started":"2025-02-18T06:21:07.331776Z","shell.execute_reply":"2025-02-18T06:21:07.368826Z"}},"outputs":[{"name":"stdout","text":"Train shape: (75952, 2)\nValidation shape: (3998, 2)\n\nTrain class distribution:\nlabel\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n\nValidation class distribution:\nlabel\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"# Training augmentations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((250,250)),  # Resize to match ConvNeXt preprocessing\n    #transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Validation and Test transforms\nval_test_transforms = transforms.Compose([\n    transforms.Resize((250,250)),  # Resize to 232 as per ConvNeXt documentation\n    #transforms.CenterCrop(224), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.370778Z","iopub.execute_input":"2025-02-18T06:21:07.371043Z","iopub.status.idle":"2025-02-18T06:21:07.377474Z","shell.execute_reply.started":"2025-02-18T06:21:07.371015Z","shell.execute_reply":"2025-02-18T06:21:07.376412Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Dataset class for training and validation\nclass AIImageDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n        image = Image.open(img_name).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        label = self.dataframe.iloc[idx, 1]\n        return image, label\n\n# Dataset class for inference (validation and test)\nclass TestAIImageDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        img_path = self.file_list[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, os.path.basename(img_path)  # Return image and filename","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.378391Z","iopub.execute_input":"2025-02-18T06:21:07.378650Z","iopub.status.idle":"2025-02-18T06:21:07.388853Z","shell.execute_reply.started":"2025-02-18T06:21:07.378627Z","shell.execute_reply":"2025-02-18T06:21:07.387931Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = AIImageDataset(train_df, root_dir=path, transform=train_transforms)\n\n# For validation, create a list of file paths and store labels separately\nval_file_list = [os.path.join(path, fname) for fname in val_df['id']]\nval_labels = val_df['label'].values  # Store labels separately for later use\nval_dataset = TestAIImageDataset(file_list=val_file_list, transform=val_test_transforms)\n\n# For testing, create a list of file paths\ntest_file_list = [os.path.join(path, fname) for fname in test['id']]\ntest_dataset = TestAIImageDataset(file_list=test_file_list, transform=val_test_transforms)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.389743Z","iopub.execute_input":"2025-02-18T06:21:07.390031Z","iopub.status.idle":"2025-02-18T06:21:07.417341Z","shell.execute_reply.started":"2025-02-18T06:21:07.390009Z","shell.execute_reply":"2025-02-18T06:21:07.415916Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 75952\nValidation dataset size: 3998\nTest dataset size: 5540\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"class DetectionModel(nn.Module):\n    def __init__(self, num_classes, backbone='Resnet-50', \n                 freeze_backbone=True, add_magnitude_channel=True, add_fft_channel=True, add_lbp_channel=True,\n                 learning_rate=1e-4, pos_weight=1):\n        super(DetectionModel, self).__init__()\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.epoch_outs = []\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.add_magnitude_channel = add_magnitude_channel\n        self.add_fft_channel = add_fft_channel\n        self.add_lbp_channel = add_lbp_channel\n        self.new_channels = sum([self.add_magnitude_channel, self.add_fft_channel, self.add_lbp_channel])\n        self.adapter = nn.Conv2d(in_channels=3+self.new_channels, out_channels=3, \n                                     kernel_size=3, stride=1, padding=1)\n        self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n        self.inplanes = self.base_model.fc.in_features\n        #self.base_model.deactive_last_layer=True\n        for param in model.parameters():\n            param.requires_grad = False\n        # for param in self.base_model.layer1.parameters():\n        #     param.requires_grad = False\n        # for param in self.base_model.layer2.parameters():\n        #     param.requires_grad = False\n        self.base_model.fc = nn.Identity()\n        #self.freeze_backbone = freeze_backbone\n        self.fc1 = nn.Linear(self.inplanes, 512)\n        self.activation=nn.ReLU() \n        self.fc2=nn.Linear(512,1)\n        \n    def _add_new_channels_worker(self, image):\n            # convert the image to grayscale\n            gray = cv.cvtColor((image.cpu().numpy() * 255).astype(np.uint8), cv.COLOR_BGR2GRAY)\n            \n            new_channels = []\n            if self.add_magnitude_channel:\n                new_channels.append(np.sqrt(cv.Sobel(gray,cv.CV_64F,1,0,ksize=7)**2 + cv.Sobel(gray,cv.CV_64F,0,1,ksize=7)**2) )\n            \n            #if fast_fourier is required, calculate it\n            if self.add_fft_channel:\n                new_channels.append(20*np.log(np.abs(np.fft.fftshift(np.fft.fft2(gray))) + 1e-9))\n            \n            #if localbinary pattern is required, calculate it\n            if self.add_lbp_channel:\n                new_channels.append(feature.local_binary_pattern(gray, 3, 6, method='uniform'))\n    \n            new_channels = np.stack(new_channels, axis=2) / 255\n            return torch.from_numpy(new_channels).to(self.device).float()\n    \n    def add_new_channels(self, images):\n            #copy the input image to avoid modifying the originalu\n            images_copied = einops.rearrange(images, \"b c h w -> b h w c\")\n            \n            # parallelize over each image in the batch using pool\n            new_channels = torch.stack([self._add_new_channels_worker(image) for image in images_copied], dim=0)\n            \n            # concatenates the new channels to the input image in the channel dimension\n            images_copied = torch.concatenate([images_copied, new_channels], dim=-1)\n            # cast img again to torch tensor and then reshape to (B, C, H, W)\n            images_copied = einops.rearrange(images_copied, \"b h w c -> b c h w\")\n            return images_copied\n        \n    def forward(self, x):\n        out = {}\n        # eventually concat the edge sharpness to the input image in the channel dimension\n        #print(x.shape)\n        if self.add_magnitude_channel or self.add_fft_channel or self.add_lbp_channel:\n            x = self.add_new_channels(x)\n        #print(x.shape)\n        # extracts the features\n        x_adapted = self.adapter(x)\n        x_adapted=self.activation(x_adapted)\n        #print(x.shape)\n        # normalizes the input image\n        #x_adapted = (x_adapted - torch.as_tensor(timm.data.constants.IMAGENET_DEFAULT_MEAN, device=self.device).view(1, -1, 1, 1)) / torch.as_tensor(timm.data.constants.IMAGENET_DEFAULT_STD, device=self.device).view(1, -1, 1, 1)\n        features = self.base_model(x_adapted)\n        \n        # outputs the logits\n        fc1_out = self.fc1(features)\n        fc1_out=self.activation(fc1_out)\n        out=self.fc2(fc1_out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.418237Z","iopub.execute_input":"2025-02-18T06:21:07.418465Z","iopub.status.idle":"2025-02-18T06:21:07.433790Z","shell.execute_reply.started":"2025-02-18T06:21:07.418444Z","shell.execute_reply":"2025-02-18T06:21:07.432857Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# print(device)\n# model = DetectionModel(num_classes=2, backbone='Resnet-50', \n#                  freeze_backbone=True, add_magnitude_channel=False, add_fft_channel=True, add_lbp_channel=True,\n#                  learning_rate=1e-4, pos_weight=1).to(device)\n\n# summary(model, input_size=(1, 3, 250, 250))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.434883Z","iopub.execute_input":"2025-02-18T06:21:07.435120Z","iopub.status.idle":"2025-02-18T06:21:07.444261Z","shell.execute_reply.started":"2025-02-18T06:21:07.435098Z","shell.execute_reply":"2025-02-18T06:21:07.443500Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = DetectionModel(num_classes=2, backbone='Resnet-50', freeze_backbone=True, add_magnitude_channel=False, add_fft_channel=True, add_lbp_channel=True,learning_rate=1e-4, pos_weight=1).to(device)\n#model = model.to(device)\nprint(device)\n# Define loss function, optimizer, and learning rate scheduler\n# optimizer = torch.optim.AdamW([\n#     {'params': model.features[-2:].parameters(), 'lr': 1e-5},  # Lower LR for backbone\n#     {'params': model.classifier.parameters(), 'lr': 1e-4}      # Higher LR for classifier\n# ])\noptimizer=torch.optim.AdamW(model.parameters(),lr=1e-4)\n\n#criterion = nn.CrossEntropyLoss()\ncriterion=nn.BCEWithLogitsLoss()\nscheduler = StepLR(optimizer, step_size=5, gamma=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.445763Z","iopub.execute_input":"2025-02-18T06:21:07.445994Z","iopub.status.idle":"2025-02-18T06:21:07.898879Z","shell.execute_reply.started":"2025-02-18T06:21:07.445972Z","shell.execute_reply":"2025-02-18T06:21:07.897493Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"\n# Training Loop\nepochs = 25\n\ntrain_losses, train_accuracies, val_losses, val_accuracies, val_f1s = [], [], [], [], []\n\nfor epoch in range(epochs):\n    # -- Training --\n    model.train()\n    epoch_loss = 0.0\n    epoch_accuracy = 0.0\n    \n    for data, label in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        data, label = data.to(device), label.to(device)\n        #print(label.shape)\n        optimizer.zero_grad()\n        output = model(data)\n        #print(output.shape)\n        label = label.unsqueeze(1).float()\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        #print(f\"Train Loss: {loss.item():.4f}\")\n        epoch_loss += loss.item()\n        #preds = output.argmax(dim=1)\n        preds = (torch.sigmoid(output) >= 0.5).int()\n        acc = (preds == label).float().mean().item()\n        epoch_accuracy += acc\n    \n    epoch_loss /= len(train_loader)\n    epoch_accuracy /= len(train_loader)\n    \n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_accuracy)\n    \n    # -- Validation --\n    model.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n    val_pred_classes = []  # To store predictions\n    val_labels_list = []   # To store true labels\n    \n    with torch.no_grad():\n        for i, (data, _) in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")):\n            data = data.to(device)\n            output = model(data)\n            \n            # Get true labels from val_df\n            batch_labels = val_labels[i * val_loader.batch_size : (i + 1) * val_loader.batch_size]\n            batch_labels = torch.tensor(batch_labels, device=device)\n            batch_labels = batch_labels.unsqueeze(1).float()\n            # Compute loss\n            loss = criterion(output, batch_labels)\n            val_loss += loss.item()\n            \n            # Compute predictions and accuracy\n            #preds = output.argmax(dim=1)\n            \n            preds = (torch.sigmoid(output) >= 0.5).int()\n            acc = (preds == batch_labels).float().mean().item()\n            val_acc += acc\n            \n            # Store predictions and true labels\n            val_pred_classes.extend(preds.cpu().numpy())\n            val_labels_list.extend(batch_labels.cpu().numpy())\n    \n    # Compute average validation metrics\n    val_loss /= len(val_loader)\n    val_acc /= len(val_loader)\n    val_f1 = f1_score(val_labels_list, val_pred_classes, average='binary')  # Binary classification\n    \n    # Append metrics\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    val_f1s.append(val_f1)\n    \n    print(\n        f\"Epoch [{epoch+1}/{epochs}] \"\n        f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_accuracy:.4f} | \"\n        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\"\n    )\n    torch.save(model.state_dict(), f\"/kaggle/working/{epoch+1}_{epoch_loss:.4f}_{val_loss:.4f}_{val_f1:.4f}.pth\")\n    # Step the learning rate scheduler\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T06:21:07.900180Z","iopub.execute_input":"2025-02-18T06:21:07.900465Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 2374/2374 [52:59<00:00,  1.34s/it]\nValidation Epoch 1: 100%|██████████| 125/125 [01:05<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/25] Train Loss: 0.1425 | Train Acc: 0.9441 | Val Loss: 0.0573 | Val Acc: 0.9795 | Val F1: 0.9797\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 2374/2374 [1:08:23<00:00,  1.73s/it]\nValidation Epoch 2: 100%|██████████| 125/125 [01:08<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/25] Train Loss: 0.0718 | Train Acc: 0.9737 | Val Loss: 0.0509 | Val Acc: 0.9830 | Val F1: 0.9829\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 2374/2374 [1:12:52<00:00,  1.84s/it]\nValidation Epoch 3: 100%|██████████| 125/125 [01:10<00:00,  1.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/25] Train Loss: 0.0526 | Train Acc: 0.9806 | Val Loss: 0.0482 | Val Acc: 0.9812 | Val F1: 0.9814\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4:  77%|███████▋  | 1825/2374 [52:06<15:46,  1.72s/it] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions and logits for the test set\nmodel.eval()\ntest_logits = []  # To store logits\ntest_pred_classes = []\n\nwith torch.no_grad():\n    for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        data = data.to(device)\n        output = model(data)  # Raw logits (before softmax)\n        \n        # Save logits\n        #test_logits.extend(output.cpu().numpy())  # Store raw logits\n        \n        # Get predicted class (0 or 1)\n        #preds = output.argmax(dim=1)\n        preds = (output >= 0.5).float()\n        test_pred_classes.extend(preds.cpu().numpy())\n\n# Convert logits to a DataFrame\n#logits_df = pd.DataFrame(test_logits, columns=['logit_class_0', 'logit_class_1'])\n#logits_df['id'] = test['id'].values  # Add image IDs for reference\n\n# Save logits to a CSV file\n#logits_df.to_csv('test_logits.csv', index=False)\n\n# Add predictions to the test DataFrame\ntest['label'] = test_pred_classes\ntest[['id', 'label']].to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Test logits saved to 'test_logits.csv'\")\nprint(\"Test predictions saved to 'submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chkpt_file=\"/kaggle/working/3_0.0957_0.0518_0.9816.pth\"\ncheckpoint = torch.load(chkpt_file, map_location=torch.device('cpu'))\npretrained_dict = checkpoint\nmodel.load_state_dict(pretrained_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions and logits for the test set using a checkpoint\nmodel.eval()\ntest_pred_classes = []\n\nwith torch.no_grad():\n    for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        data = data.to(device)\n        output = model(data)\n        #print(output)\n        output=torch.sigmoid(output)\n        #print(output)\n        preds = (output >= 0.5).int()\n        #print(preds)\n        test_pred_classes.extend(preds.cpu().numpy().flatten())\ntest['label'] = test_pred_classes\ntest[['id', 'label']].to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Test predictions saved to 'submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.read_csv('submission.csv')['label'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}