{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91198,"databundleVersionId":10884264,"sourceType":"competition"},{"sourceId":10550636,"sourceType":"datasetVersion","datasetId":6412205}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torchinfo\n!pip install scikit-image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:28:07.925081Z","iopub.execute_input":"2025-02-18T18:28:07.925400Z","iopub.status.idle":"2025-02-18T18:28:19.049566Z","shell.execute_reply.started":"2025-02-18T18:28:07.925371Z","shell.execute_reply":"2025-02-18T18:28:19.048342Z"}},"outputs":[{"name":"stdout","text":"Collecting torchinfo\n  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.8.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting scikit-image\n  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lazy-loader>=0.4\n  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\nCollecting imageio!=2.35.0,>=2.33\n  Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.8/315.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/site-packages (from scikit-image) (24.2)\nRequirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.10/site-packages (from scikit-image) (11.1.0)\nRequirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.10/site-packages (from scikit-image) (1.15.1)\nCollecting tifffile>=2022.8.12\n  Downloading tifffile-2025.1.10-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/site-packages (from scikit-image) (2.0.2)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/site-packages (from scikit-image) (3.4.2)\nInstalling collected packages: tifffile, lazy-loader, imageio, scikit-image\nSuccessfully installed imageio-2.37.0 lazy-loader-0.4 scikit-image-0.25.2 tifffile-2025.1.10\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\n#from torch.nn.Functional\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport cv2 as cv\nfrom torchinfo import summary\nimport einops\nfrom skimage import feature\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:01:19.637352Z","iopub.execute_input":"2025-02-19T04:01:19.637748Z","iopub.status.idle":"2025-02-19T04:01:37.325638Z","shell.execute_reply.started":"2025-02-19T04:01:19.637713Z","shell.execute_reply":"2025-02-19T04:01:37.323746Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths to dataset files\npath = '/kaggle/input/ai-vs-human-generated-dataset'\ntrain_csv = '/kaggle/input/detect-ai-vs-human-generated-images/train.csv'\ntest_csv = '/kaggle/input/detect-ai-vs-human-generated-images/test.csv'\n\n# Load the training and test datasets\ntrain = pd.read_csv(train_csv)\ntest = pd.read_csv(test_csv)\n\n# Print dataset shapes\nprint(f'Training dataset shape: {train.shape}')\nprint(f'Test dataset shape: {test.shape}')\n\n# Preprocess column names for consistency\ntrain = train[['file_name', 'label']]\ntrain.columns = ['id', 'label']\n\n# Display columns for reference\nprint(\"Train columns:\", train.columns)\nprint(\"Test columns:\", test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:01:51.648682Z","iopub.execute_input":"2025-02-19T04:01:51.649039Z","iopub.status.idle":"2025-02-19T04:01:51.867082Z","shell.execute_reply.started":"2025-02-19T04:01:51.649009Z","shell.execute_reply":"2025-02-19T04:01:51.865537Z"}},"outputs":[{"name":"stdout","text":"Training dataset shape: (79950, 3)\nTest dataset shape: (5540, 1)\nTrain columns: Index(['id', 'label'], dtype='object')\nTest columns: Index(['id'], dtype='object')\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"print(\"To check the data distribution for training\")\ntrain['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:28:30.331447Z","iopub.execute_input":"2025-02-18T18:28:30.331664Z","iopub.status.idle":"2025-02-18T18:28:30.341816Z","shell.execute_reply.started":"2025-02-18T18:28:30.331643Z","shell.execute_reply":"2025-02-18T18:28:30.340827Z"}},"outputs":[{"name":"stdout","text":"To check the data distribution for training\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"label\n1    39975\n0    39975\nName: count, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Split the training data into training and validation sets (95% train, 5% validation)\ntrain_df, val_df = train_test_split(\n    train, \n    test_size=0.05, \n    random_state=42,  \n    stratify=train['label'] \n)\n\n# Print shapes of the splits\nprint(f'Train shape: {train_df.shape}')\nprint(f'Validation shape: {val_df.shape}')\n\n# Check class distribution in both sets\nprint(\"\\nTrain class distribution:\")\nprint(train_df['label'].value_counts(normalize=True))\n\nprint(\"\\nValidation class distribution:\")\nprint(val_df['label'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:01:57.519306Z","iopub.execute_input":"2025-02-19T04:01:57.519665Z","iopub.status.idle":"2025-02-19T04:01:57.598617Z","shell.execute_reply.started":"2025-02-19T04:01:57.519635Z","shell.execute_reply":"2025-02-19T04:01:57.597406Z"}},"outputs":[{"name":"stdout","text":"Train shape: (75952, 2)\nValidation shape: (3998, 2)\n\nTrain class distribution:\nlabel\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n\nValidation class distribution:\nlabel\n0    0.5\n1    0.5\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Training augmentations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((250,250)),  # Resize to match ConvNeXt preprocessing\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Validation and Test transforms\nval_test_transforms = transforms.Compose([\n    transforms.Resize((250,250)),  # Resize to 232 as per ConvNeXt documentation\n    transforms.CenterCrop(224), \n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:02:08.771885Z","iopub.execute_input":"2025-02-19T04:02:08.772293Z","iopub.status.idle":"2025-02-19T04:02:08.782452Z","shell.execute_reply.started":"2025-02-19T04:02:08.772243Z","shell.execute_reply":"2025-02-19T04:02:08.779124Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Dataset class for training and validation\nclass AIImageDataset(Dataset):\n    def __init__(self, dataframe, root_dir, transform=None):\n        self.dataframe = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n        image = Image.open(img_name).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        label = self.dataframe.iloc[idx, 1]\n        return image, label\n\n# Dataset class for inference (validation and test)\nclass TestAIImageDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n        img_path = self.file_list[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, os.path.basename(img_path)  # Return image and filename","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:02:10.988741Z","iopub.execute_input":"2025-02-19T04:02:10.989126Z","iopub.status.idle":"2025-02-19T04:02:10.999075Z","shell.execute_reply.started":"2025-02-19T04:02:10.989090Z","shell.execute_reply":"2025-02-19T04:02:10.996968Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = AIImageDataset(train_df, root_dir=path, transform=train_transforms)\n\n# For validation, create a list of file paths and store labels separately\nval_file_list = [os.path.join(path, fname) for fname in val_df['id']]\nval_labels = val_df['label'].values  # Store labels separately for later use\nval_dataset = TestAIImageDataset(file_list=val_file_list, transform=val_test_transforms)\n\n# For testing, create a list of file paths\ntest_file_list = [os.path.join(path, fname) for fname in test['id']]\ntest_dataset = TestAIImageDataset(file_list=test_file_list, transform=val_test_transforms)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:02:12.989458Z","iopub.execute_input":"2025-02-19T04:02:12.989907Z","iopub.status.idle":"2025-02-19T04:02:13.017163Z","shell.execute_reply.started":"2025-02-19T04:02:12.989873Z","shell.execute_reply":"2025-02-19T04:02:13.013573Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 75952\nValidation dataset size: 3998\nTest dataset size: 5540\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class DetectionModel(nn.Module):\n    def __init__(self, num_classes, backbone='Resnet-50', \n                 freeze_backbone=True, add_magnitude_channel=True, add_fft_channel=True, add_lbp_channel=True,\n                 learning_rate=1e-4, pos_weight=1):\n        super(DetectionModel, self).__init__()\n        self.num_classes = num_classes\n        self.learning_rate = learning_rate\n        self.epoch_outs = []\n        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n        self.add_magnitude_channel = add_magnitude_channel\n        self.add_fft_channel = add_fft_channel\n        self.add_lbp_channel = add_lbp_channel\n        self.new_channels = sum([self.add_magnitude_channel, self.add_fft_channel, self.add_lbp_channel])\n        self.adapter = nn.Conv2d(in_channels=3+self.new_channels, out_channels=3, \n                                     kernel_size=3, stride=1, padding=1)\n        self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n        self.inplanes = self.base_model.fc.in_features\n        #self.base_model.deactive_last_layer=True\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n        # for param in self.base_model.layer1.parameters():\n        #     param.requires_grad = False\n        # for param in self.base_model.layer2.parameters():\n        #     param.requires_grad = False\n        self.base_model.fc = nn.Identity()\n        #self.freeze_backbone = freeze_backbone\n        self.fc1 = nn.Linear(self.inplanes, 512)\n        self.activation=nn.ReLU() \n        self.fc2=nn.Linear(512,2)\n        \n    def _add_new_channels_worker(self, image):\n            # convert the image to grayscale\n            gray = cv.cvtColor((image.cpu().numpy() * 255).astype(np.uint8), cv.COLOR_BGR2GRAY)\n            \n            new_channels = []\n            if self.add_magnitude_channel:\n                new_channels.append(np.sqrt(cv.Sobel(gray,cv.CV_64F,1,0,ksize=7)**2 + cv.Sobel(gray,cv.CV_64F,0,1,ksize=7)**2) )\n            \n            #if fast_fourier is required, calculate it\n            if self.add_fft_channel:\n                new_channels.append(20*np.log(np.abs(np.fft.fftshift(np.fft.fft2(gray))) + 1e-9))\n            \n            #if localbinary pattern is required, calculate it\n            if self.add_lbp_channel:\n                new_channels.append(feature.local_binary_pattern(gray, 3, 6, method='uniform'))\n    \n            new_channels = np.stack(new_channels, axis=2) / 255\n            return torch.from_numpy(new_channels).to(self.device).float()\n    \n    def add_new_channels(self, images):\n            #copy the input image to avoid modifying the originalu\n            images_copied = einops.rearrange(images, \"b c h w -> b h w c\")\n            \n            # parallelize over each image in the batch using pool\n            new_channels = torch.stack([self._add_new_channels_worker(image) for image in images_copied], dim=0)\n            \n            # concatenates the new channels to the input image in the channel dimension\n            images_copied = torch.concatenate([images_copied, new_channels], dim=-1)\n            # cast img again to torch tensor and then reshape to (B, C, H, W)\n            images_copied = einops.rearrange(images_copied, \"b h w c -> b c h w\")\n            return images_copied\n        \n    def forward(self, x):\n        out = {}\n        # eventually concat the edge sharpness to the input image in the channel dimension\n        #print(x.shape)\n        if self.add_magnitude_channel or self.add_fft_channel or self.add_lbp_channel:\n            x = self.add_new_channels(x)\n        #print(x.shape)\n        # extracts the features\n        x_adapted = self.adapter(x)\n        x_adapted=self.activation(x_adapted)\n        #print(x.shape)\n        # normalizes the input image\n        x_adapted = (x_adapted - torch.as_tensor(timm.data.constants.IMAGENET_DEFAULT_MEAN, device=self.device).view(1, -1, 1, 1)) / torch.as_tensor(timm.data.constants.IMAGENET_DEFAULT_STD, device=self.device).view(1, -1, 1, 1)\n        features = self.base_model(x_adapted)\n        \n        # outputs the logits\n        fc1_out = self.fc1(features)\n        fc1_out=self.activation(fc1_out)\n        out=self.fc2(fc1_out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:02:15.938124Z","iopub.execute_input":"2025-02-19T04:02:15.938555Z","iopub.status.idle":"2025-02-19T04:02:15.955214Z","shell.execute_reply.started":"2025-02-19T04:02:15.938517Z","shell.execute_reply":"2025-02-19T04:02:15.953087Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n# print(device)\n# model = DetectionModel(num_classes=2, backbone='Resnet-50', \n#                  freeze_backbone=True, add_magnitude_channel=False, add_fft_channel=True, add_lbp_channel=True,\n#                  learning_rate=1e-4, pos_weight=1).to(device)\n\n# summary(model, input_size=(1, 3, 250, 250))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:28:30.449158Z","iopub.execute_input":"2025-02-18T18:28:30.449352Z","iopub.status.idle":"2025-02-18T18:28:30.458681Z","shell.execute_reply.started":"2025-02-18T18:28:30.449332Z","shell.execute_reply":"2025-02-18T18:28:30.457768Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = DetectionModel(num_classes=2, backbone='Resnet-50', freeze_backbone=True, add_magnitude_channel=False, add_fft_channel=True, add_lbp_channel=True,learning_rate=1e-4, pos_weight=1).to(device)\n#model = model.to(device)\nprint(device)\n# Define loss function, optimizer, and learning rate scheduler\n# optimizer = torch.optim.AdamW([\n#     {'params': model.features[-2:].parameters(), 'lr': 1e-5},  # Lower LR for backbone\n#     {'params': model.classifier.parameters(), 'lr': 1e-4}      # Higher LR for classifier\n# ])\noptimizer=torch.optim.AdamW(model.parameters(),lr=1e-4)\n\ncriterion = nn.CrossEntropyLoss()\n#criterion=nn.BCEWithLogitsLoss()\nscheduler = StepLR(optimizer, step_size=5, gamma=0.7)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:02:22.123784Z","iopub.execute_input":"2025-02-19T04:02:22.124349Z","iopub.status.idle":"2025-02-19T04:02:23.705852Z","shell.execute_reply.started":"2025-02-19T04:02:22.124308Z","shell.execute_reply":"2025-02-19T04:02:23.703603Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 289MB/s]\n","output_type":"stream"},{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\n# Training Loop\nepochs = 15\n\ntrain_losses, train_accuracies, val_losses, val_accuracies, val_f1s = [], [], [], [], []\n\nfor epoch in range(epochs):\n    # -- Training --\n    model.train()\n    epoch_loss = 0.0\n    epoch_accuracy = 0.0\n    \n    for data, label in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n        data, label = data.to(device), label.to(device)\n        #print(label.shape)\n        optimizer.zero_grad()\n        output = model(data)\n        #print(output.shape)\n        #label = label.unsqueeze(1).float()\n        loss = criterion(output, label)\n        loss.backward()\n        optimizer.step()\n        #print(f\"Train Loss: {loss.item():.4f}\")\n        epoch_loss += loss.item()\n        preds = output.argmax(dim=1)\n        #preds = (torch.sigmoid(output) >= 0.5).int()\n        acc = (preds == label).float().mean().item()\n        epoch_accuracy += acc\n    \n    epoch_loss /= len(train_loader)\n    epoch_accuracy /= len(train_loader)\n    \n    train_losses.append(epoch_loss)\n    train_accuracies.append(epoch_accuracy)\n    \n    # -- Validation --\n    model.eval()\n    val_loss = 0.0\n    val_acc = 0.0\n    val_pred_classes = []  # To store predictions\n    val_labels_list = []   # To store true labels\n    \n    with torch.no_grad():\n        for i, (data, _) in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")):\n            data = data.to(device)\n            output = model(data)\n            \n            # Get true labels from val_df\n            batch_labels = val_labels[i * val_loader.batch_size : (i + 1) * val_loader.batch_size]\n            batch_labels = torch.tensor(batch_labels, device=device)\n            #batch_labels = batch_labels.unsqueeze(1).float()\n            # Compute loss\n            loss = criterion(output, batch_labels)\n            val_loss += loss.item()\n            \n            # Compute predictions and accuracy\n            preds = output.argmax(dim=1)\n            \n            #preds = (torch.sigmoid(output) >= 0.5).int()\n            acc = (preds == batch_labels).float().mean().item()\n            val_acc += acc\n            \n            # Store predictions and true labels\n            val_pred_classes.extend(preds.cpu().numpy())\n            val_labels_list.extend(batch_labels.cpu().numpy())\n    \n    # Compute average validation metrics\n    val_loss /= len(val_loader)\n    val_acc /= len(val_loader)\n    val_f1 = f1_score(val_labels_list, val_pred_classes, average='binary')  # Binary classification\n    \n    # Append metrics\n    val_losses.append(val_loss)\n    val_accuracies.append(val_acc)\n    val_f1s.append(val_f1)\n    \n    print(\n        f\"Epoch [{epoch+1}/{epochs}] \"\n        f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_accuracy:.4f} | \"\n        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\"\n    )\n    torch.save(model.state_dict(), f\"/kaggle/working/{epoch+1}_{epoch_loss:.4f}_{val_loss:.4f}_{val_f1:.4f}.pth\")\n    # Step the learning rate scheduler\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T18:28:31.588576Z","iopub.execute_input":"2025-02-18T18:28:31.588865Z","execution_failed":"2025-02-19T03:23:23.383Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 2374/2374 [44:30<00:00,  1.12s/it]\nValidation Epoch 1: 100%|██████████| 125/125 [01:04<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15] Train Loss: 0.3112 | Train Acc: 0.8682 | Val Loss: 0.2304 | Val Acc: 0.9087 | Val F1: 0.9085\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 2374/2374 [44:10<00:00,  1.12s/it]\nValidation Epoch 2: 100%|██████████| 125/125 [01:04<00:00,  1.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/15] Train Loss: 0.2534 | Train Acc: 0.8925 | Val Loss: 0.2092 | Val Acc: 0.9195 | Val F1: 0.9201\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 2374/2374 [43:38<00:00,  1.10s/it]\nValidation Epoch 3: 100%|██████████| 125/125 [01:09<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/15] Train Loss: 0.2278 | Train Acc: 0.9044 | Val Loss: 0.1968 | Val Acc: 0.9200 | Val F1: 0.9198\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 2374/2374 [45:07<00:00,  1.14s/it]\nValidation Epoch 4: 100%|██████████| 125/125 [01:09<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/15] Train Loss: 0.2112 | Train Acc: 0.9112 | Val Loss: 0.1861 | Val Acc: 0.9272 | Val F1: 0.9255\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 2374/2374 [45:12<00:00,  1.14s/it]\nValidation Epoch 5: 100%|██████████| 125/125 [01:07<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/15] Train Loss: 0.1982 | Train Acc: 0.9188 | Val Loss: 0.1829 | Val Acc: 0.9305 | Val F1: 0.9322\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 2374/2374 [45:41<00:00,  1.15s/it]\nValidation Epoch 6: 100%|██████████| 125/125 [01:08<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/15] Train Loss: 0.1851 | Train Acc: 0.9241 | Val Loss: 0.1593 | Val Acc: 0.9400 | Val F1: 0.9400\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 2374/2374 [45:28<00:00,  1.15s/it]\nValidation Epoch 7: 100%|██████████| 125/125 [01:09<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/15] Train Loss: 0.1764 | Train Acc: 0.9286 | Val Loss: 0.1578 | Val Acc: 0.9372 | Val F1: 0.9378\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 2374/2374 [45:04<00:00,  1.14s/it]\nValidation Epoch 8: 100%|██████████| 125/125 [01:09<00:00,  1.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/15] Train Loss: 0.1703 | Train Acc: 0.9319 | Val Loss: 0.1568 | Val Acc: 0.9415 | Val F1: 0.9411\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 2374/2374 [45:28<00:00,  1.15s/it]\nValidation Epoch 9: 100%|██████████| 125/125 [01:07<00:00,  1.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/15] Train Loss: 0.1621 | Train Acc: 0.9355 | Val Loss: 0.1483 | Val Acc: 0.9432 | Val F1: 0.9431\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 2374/2374 [45:06<00:00,  1.14s/it]\nValidation Epoch 10: 100%|██████████| 125/125 [01:07<00:00,  1.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/15] Train Loss: 0.1583 | Train Acc: 0.9361 | Val Loss: 0.1633 | Val Acc: 0.9339 | Val F1: 0.9320\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 11: 100%|██████████| 2374/2374 [45:00<00:00,  1.14s/it]\nValidation Epoch 11: 100%|██████████| 125/125 [01:08<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/15] Train Loss: 0.1527 | Train Acc: 0.9388 | Val Loss: 0.1513 | Val Acc: 0.9412 | Val F1: 0.9411\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 12:  63%|██████▎   | 1486/2374 [27:53<15:57,  1.08s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#!rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T03:23:23.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions and logits for the test set\nmodel.eval()\ntest_logits = []  # To store logits\ntest_pred_classes = []\n\nwith torch.no_grad():\n    for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        data = data.to(device)\n        output = model(data)  # Raw logits (before softmax)\n        \n        # Save logits\n        #test_logits.extend(output.cpu().numpy())  # Store raw logits\n        \n        # Get predicted class (0 or 1)\n        preds = output.argmax(dim=1)\n        #preds = (output >= 0.5).float()\n        test_pred_classes.extend(preds.cpu().numpy())\n\n# Convert logits to a DataFrame\n#logits_df = pd.DataFrame(test_logits, columns=['logit_class_0', 'logit_class_1'])\n#logits_df['id'] = test['id'].values  # Add image IDs for reference\n\n# Save logits to a CSV file\n#logits_df.to_csv('test_logits.csv', index=False)\n\n# Add predictions to the test DataFrame\ntest['label'] = test_pred_classes\ntest[['id', 'label']].to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Test logits saved to 'test_logits.csv'\")\nprint(\"Test predictions saved to 'submission.csv'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-19T03:23:23.383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"chkpt_file=\"/kaggle/working/2_0.2534_0.2092_0.9201.pth\"\ncheckpoint = torch.load(chkpt_file, map_location=torch.device('cpu'))\npretrained_dict = checkpoint\nmodel.load_state_dict(pretrained_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:21:05.817753Z","iopub.execute_input":"2025-02-19T04:21:05.818169Z","iopub.status.idle":"2025-02-19T04:21:06.011972Z","shell.execute_reply.started":"2025-02-19T04:21:05.818135Z","shell.execute_reply":"2025-02-19T04:21:06.010883Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-eae65c465ed5>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(chkpt_file, map_location=torch.device('cpu'))\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Generate predictions and logits for the test set using a checkpoint\nmodel.eval()\ntest_pred_classes = []\n\nwith torch.no_grad():\n    for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n        data = data.to(device)\n        output = model(data)\n        #print(output)\n        #output=torch.sigmoid(output)\n        #print(output)\n        #preds = (output >= 0.5).int()\n        #print(preds)\n        preds = output.argmax(dim=1)\n        test_pred_classes.extend(preds.cpu().numpy().flatten())\ntest['label'] = test_pred_classes\ntest[['id', 'label']].to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"Test predictions saved to 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:21:08.520906Z","iopub.execute_input":"2025-02-19T04:21:08.521351Z"}},"outputs":[{"name":"stderr","text":"Generating Test Predictions:  13%|█▎        | 23/174 [01:55<11:53,  4.73s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"pd.read_csv('submission.csv')['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T04:18:20.880459Z","iopub.execute_input":"2025-02-19T04:18:20.880970Z","iopub.status.idle":"2025-02-19T04:18:20.915140Z","shell.execute_reply.started":"2025-02-19T04:18:20.880922Z","shell.execute_reply":"2025-02-19T04:18:20.913682Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"label\n0    4157\n1    1383\nName: count, dtype: int64"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}